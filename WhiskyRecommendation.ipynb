{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages \n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit,CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml import Pipeline, PipelineModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset path\n",
    "\n",
    "def processing(df_raw):                 \n",
    "    df_raw = df_raw.withColumn(\"rating\", df_raw[\"score\"].cast(\"float\")).drop(df_raw[\"score\"])\n",
    "    # transform user_name and whisky_name string to index using spark StringIndexer function\n",
    "    whiskyIndexer = StringIndexer(inputCol=\"whisky_name\", outputCol=\"item\",handleInvalid='error') # create indexer for asins\n",
    "    userIndexer = StringIndexer(inputCol='user_name',outputCol='userid',handleInvalid='error') # create indexer for user\n",
    "    whiskyIndexed = whiskyIndexer.fit(df_raw).transform(df_raw) # apply whisky_name indexer\n",
    "    userIndexed = userIndexer.fit(whiskyIndexed).transform(whiskyIndexed) # apply user indexer\n",
    "    df_indexed = userIndexed.drop('whisky_name').drop('user_name') # remove old columns with alphanumeric strings\n",
    "    return df_indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df_indexed):  \n",
    "    # 70-30 train-test split\n",
    "    (df_train, df_test) = df_indexed.randomSplit([0.7,0.3])\n",
    "    # cache them in memory across clusters since we access this data frequently \n",
    "    df_train.cache() \n",
    "    df_test.cache()\n",
    "    print(\"==============訓練中==============\")\n",
    "    # Display dataset size\n",
    "#     print('Train set size: {}'.format(df_train.count()))\n",
    "#     print('Test set size: {}'.format(df_test.count()))\n",
    "    return df_train,df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "def recommendwhiskys(model,df_train,user, num_rec):\n",
    "    # Create a dataset with distinct whisky as one column and the user of interest as another column\n",
    "    itemsuser = df_train.select(\"item\").distinct().withColumn(\"userid\", lit(user))\n",
    "#     itemsuser.show(n=3)\n",
    "\n",
    "    # filter out games that user has already rated \n",
    "    whiskysrated = df_train.filter(df_train.userid == user).select(\"item\", \"userid\")\n",
    "\n",
    "    # apply trained recommender system\n",
    "    predictions = model.transform(itemsuser.subtract(whiskysrated)).dropna().orderBy(\"prediction\", ascending=False).limit(num_rec).select(\"item\", \"prediction\")\n",
    "#     predictions.show()\n",
    "    \n",
    "    # convert index back to original whisky \n",
    "    converter = IndexToString(inputCol=\"item\", outputCol=\"recommend_whisky\")\n",
    "    converted = converter.transform(predictions)\n",
    "    converted.show()\n",
    "    return converted\n",
    "    \n",
    "   \n",
    "    \n",
    "#     return converted\n",
    "    \n",
    "    \n",
    "      #轉成rdd去做推薦\n",
    "#     convertedtordd = converted.rdd.take(num_rec) \n",
    "#     convertedtordd.take()\n",
    "#     for item in convertedtordd:\n",
    "#         print(item)\n",
    "# #         print(type(item))\n",
    "#         print(\"推薦whisky:{}, 推薦評分:{}\".format(str(item[2]),item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#輸入酒款id 推薦給3個人\n",
    "#再從人的id去看上一個程式碼可每人推3個酒款\n",
    "#最後產出的9個酒款便是最終前端產生的結果\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "def recommendwhiskys_user(model,df_train,item, num_rec):\n",
    "    # Create a dataset with distinct whisky as one column and the user of interest as another column\n",
    "    useritems = df_train.select(\"userid\").distinct().withColumn(\"item\", lit(item))\n",
    "#     useritems.show(n=3)\n",
    "\n",
    "    # filter out games that user has already rated  (曾經評論過的)\n",
    "    whiskysrated = df_train.filter(df_train.item == item).select(\"userid\", \"item\")\n",
    "#     whiskysrated.show()\n",
    "    # apply trained recommender system  (以下為預測使用者可能喜歡的酒款)\n",
    "    predictions = model.transform(useritems.subtract(whiskysrated)).dropna().orderBy(\"prediction\", ascending=False).limit(num_rec).select(\"userid\", \"prediction\")\n",
    "#     predictions.show()\n",
    "    \n",
    "    # convert index back to original whisky \n",
    "    converter = IndexToString(inputCol=\"userid\", outputCol=\"recommend_user\")\n",
    "    converted = converter.transform(predictions)\n",
    "#     converted.show()\n",
    "    convertedtordd = converted.rdd.take(num_rec)\n",
    "#     id_list = [] \n",
    "    for item in convertedtordd:\n",
    "        result = recommendwhiskys(model,df_train,item[0],num_rec)\n",
    "    return result \n",
    "        \n",
    "#         print(item)\n",
    "#         id_list.append(item[0])\n",
    "#         print(\"使用者id:{}\".format(str(item[0])))\n",
    "#         print(\"推薦給使用者:{}, 推薦評分:{}\".format(str(item[2]),item[1]))\n",
    "#     print (id_list)\n",
    "#     data= []\n",
    "#     for i in id_list :\n",
    "#         result = recommendwhiskys(model,df_train,i,num_rec)\n",
    "#         print(result)\n",
    "#         data.append(result)\n",
    "#     data.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============訓練中==============\n",
      "=========輸入推薦項目數量==============\n",
      "=========輸入威士忌id==============\n",
      "2500\n",
      "+------+----------+--------------------+\n",
      "|  item|prediction|    recommend_whisky|\n",
      "+------+----------+--------------------+\n",
      "|8550.0|  6.852953|sivo-rebel-le-moo...|\n",
      "|6089.0| 6.7254953|old-elk-straight-...|\n",
      "+------+----------+--------------------+\n",
      "\n",
      "+------+----------+--------------------+\n",
      "|  item|prediction|    recommend_whisky|\n",
      "+------+----------+--------------------+\n",
      "|8550.0|  6.732022|sivo-rebel-le-moo...|\n",
      "|4989.0| 6.1625566|belgrove-rye-whiskey|\n",
      "+------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw = spark.read.csv(\"hdfs://devenv/user/spark/whisky/user_whisky_score.csv\",header = True)        \n",
    "df_train = train(processing(df_raw))[0]\n",
    "    \n",
    "from pyspark.ml.tuning import TrainValidationSplitModel\n",
    "model = TrainValidationSplitModel.load(\"hdfs://devenv/user/spark/whisky/AlSmodel\")\n",
    "\n",
    "print(\"=========輸入推薦項目數量==============\")\n",
    "usernum=int(2)\n",
    "print(\"=========輸入威士忌id==============\")\n",
    "itemnum = int(input())\n",
    "result = recommendwhiskys_user(model,df_train,itemnum,usernum)\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(\n",
    "#     columns=['item', 'prediction','recommend_whisky'])\n",
    "\n",
    "# dff = df = pd.DataFrame(all,columns=['酒名', '酒譜', 'url','圖片','步驟', '介紹', '評論'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+\n",
      "|  item|prediction|    recommend_whisky|\n",
      "+------+----------+--------------------+\n",
      "|8550.0|  6.732022|sivo-rebel-le-moo...|\n",
      "|4989.0| 6.1625566|belgrove-rye-whiskey|\n",
      "+------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============訓練模型==============\n",
      "==============訓練中==============\n",
      "==========載入模型==============\n",
      "==========進行推薦===============\n",
      "=========輸入推薦項目數量==============\n",
      "=========輸入威士忌id==============\n",
      "3000\n",
      "+------+----------+--------------------+\n",
      "|  item|prediction|    recommend_whisky|\n",
      "+------+----------+--------------------+\n",
      "|5828.0| 6.4259686|nikka-single-cask...|\n",
      "|6982.0| 6.2807374|bruichladdich-18-...|\n",
      "+------+----------+--------------------+\n",
      "\n",
      "+------+----------+--------------------+\n",
      "|  item|prediction|    recommend_whisky|\n",
      "+------+----------+--------------------+\n",
      "|8550.0|  6.852953|sivo-rebel-le-moo...|\n",
      "|6089.0| 6.7254953|old-elk-straight-...|\n",
      "+------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"==============訓練模型==============\")\n",
    "    df_raw = spark.read.csv(\"hdfs://devenv/user/spark/whisky/user_whisky_score.csv\",header = True)        \n",
    "    \n",
    "    df_train = train(processing(df_raw))[0]\n",
    "    \n",
    "    \n",
    "    print(\"==========載入模型==============\")\n",
    "    \n",
    "    from pyspark.ml.tuning import TrainValidationSplitModel\n",
    "    model = TrainValidationSplitModel.load(\"hdfs://devenv/user/spark/whisky/AlSmodel\")\n",
    "\n",
    "    print(\"==========進行推薦===============\")\n",
    "    \n",
    "    print(\"=========輸入推薦項目數量==============\")\n",
    "    usernum=int(2)\n",
    "    print(\"=========輸入威士忌id==============\")\n",
    "    itemnum = int(input())\n",
    "    result = recommendwhiskys_user(model,df_train,itemnum,usernum)\n",
    "    \n",
    "    \n",
    "    #貯存所有結果至json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item: double, prediction: float, recommend_whisky: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-02800ca79ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/Desktop/spark101/movies/data40n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"records\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m##################insert to mongo##########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "df3 = df2.toPandas() \n",
    "df3.to_json('~/Desktop/spark101/movies/data40n',orient = \"records\")\n",
    "\n",
    "##################insert to mongo##########################\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "\n",
    "myclient = MongoClient(\"mongodb://10.120.26.13:27017/\")\n",
    "db = myclient[\"MOVIE\"]\n",
    "Collection = db[\"copy_name\"]\n",
    "\n",
    "with open('/home/spark/Desktop/spark101/movies/data40n') as file:\n",
    "    file_data = json.load(file)\n",
    "\n",
    "if isinstance(file_data, list):\n",
    "    Collection.insert_many(file_data)\n",
    "else:\n",
    "    Collection.insert_one(file_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
